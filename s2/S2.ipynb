{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the `libs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJptKBxALl-u",
    "outputId": "a4bddfbd-78c1-47e5-8b6a-c2aaf654279f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for `GPU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00Owi1LBNY8L",
    "outputId": "9b59ed1f-019f-402a-98c4-337babe98c28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if a GPU is available (if a GPU is available, CUDA would be loaded)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# get a reference to the available GPU instance\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQZaZRGcNLtr",
    "outputId": "dc8ff540-bbc8-4224-f335-a6a41648ab95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 118773154.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 3183759.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1648877/1648877 [00:00<00:00, 34290559.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 9449667.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of input data points in each batch\n",
    "batch_size = 128\n",
    "\n",
    "# instantiate a DataLoader for training\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    # download MNIST dataset, and only the Training dataset\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                    # pre-processing that each chunk of input data will go through\n",
    "                    transform=transforms.Compose([\n",
    "                        # convert to a Tensor\n",
    "                        transforms.ToTensor(),\n",
    "                        # data in the tensor will be normalized to the below\n",
    "                        # mentioned Mean and Standard Deviation\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    # randomly shuffle input data for each batch\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# instantiate a DataLoader for testing\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    # download MNIST dataset, and only the Testing dataset this time\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                        # convert to a Tensor\n",
    "                        transforms.ToTensor(),\n",
    "                        # use the same mu and sigma values as for training data\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3gEjf-xMb-N"
   },
   "source": [
    "# Some Notes on our naive model\n",
    "\n",
    "We are going to write a network based on what we have learnt so far.\n",
    "\n",
    "The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Sir2LmSVLr_4"
   },
   "outputs": [],
   "source": [
    "class FirstDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FirstDNN, self).__init__()\n",
    "    \n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)       # r_in:1, n_in:28, j_in:1, s:1, p:1, r_out:3, n_out:28, j_out:1\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)      # r_in:3, n_in:28, j_in:1, s:1, p:1, r_out:5, n_out:28, j_out:1\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)                   # r_in:5, n_in:28, j_in:1, s:2, p:0, r_out:6, n_out:14, j_out:2\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)     # r_in:6,  n_in:14, j_in:2, s:1, p:1, r_out:10, n_out:14, j_out:2\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)    # r_in:10, n_in:14, j_in:2, s:1, p:1, r_out:14, n_out:14, j_out:2\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)                   # r_in:14, n_in:14, j_in:2, s:2, p:0, r_out:16, n_out:7,  j_out:4\n",
    "        \n",
    "        # Block 3\n",
    "        self.conv5 = nn.Conv2d(256, 512, 3)               # r_in:16, n_in:7, j_in:4, s:1, p:0, r_out:24, n_out:5, j_out:4\n",
    "        self.conv6 = nn.Conv2d(512, 1024, 3)              # r_in:24, n_in:5, j_in:4, s:1, p:0, r_out:32, n_out:3, j_out:4\n",
    "        self.conv7 = nn.Conv2d(1024, 10, 3)               # r_in:32, n_in:3, j_in:4, s:1, p:0, r_out:40, n_out:1, j_out:4\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1 (CONV -> RELU -> CONV -> RELU -> MP)\n",
    "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
    "        \n",
    "        # Block 2 (CONV -> RELU -> CONV -> RELU -> MP)\n",
    "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
    "        \n",
    "        # Block 3 (CONV -> RELU -> CONV -> RELU)\n",
    "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
    "\n",
    "        # Final Layer (CONV)\n",
    "        x = self.conv7(x)\n",
    "        \n",
    "        # Flatten and Reshaping it to 10 output (Vector)\n",
    "        x = x.view(-1, 10)\n",
    "        \n",
    "        # Applying Softmax on the vector to get the intensity of 10 different outputs \n",
    "        return F.log_softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sxICO4TTNt2H"
   },
   "outputs": [],
   "source": [
    "# send the model to the GPU device\n",
    "model = FirstDNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M__MtFIYNwXa",
    "outputId": "e9c6eaa9-e672-4966-9300-40b7a642d724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             320\n",
      "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
      "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
      "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
      "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
      "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
      "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
      "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
      "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
      "================================================================\n",
      "Total params: 6,379,786\n",
      "Trainable params: 6,379,786\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.51\n",
      "Params size (MB): 24.34\n",
      "Estimated Total Size (MB): 25.85\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3804/4093577519.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "# print a summary of the above model, given the input_size\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Train and Test wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "g_vlC-bdNzo1"
   },
   "outputs": [],
   "source": [
    "# Basic workflows for training and testing of the model\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # tell the model that we are in the training phase\n",
    "    model.train()\n",
    "\n",
    "    # Prepare to iterate the progress bar over the input training data loader\n",
    "    pbar = tqdm(train_loader)\n",
    "\n",
    "    # 5-step process of training, for each iteration of input data batch\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        # initialize the training input data and labels to the device memory\n",
    "        data, target = data.to(device), target.to(device)\n",
    "    \n",
    "        # Step-1: intialize the optimizer parameters so that gradients of other\n",
    "        # epochs dont mess with the current epoch\n",
    "        optimizer.zero_grad()\n",
    "        # Step-2: do a forward pass through the model using the current data batch\n",
    "        output = model(data)\n",
    "        # Step-3: compute the loss after the forward pass\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Step-4: compute the gradients for all tensors in the model using backpropagation\n",
    "        loss.backward()\n",
    "        # Step-5: iterate over all parameters (tensors) it is supposed to update\n",
    "        # and use their internally stored grad to update their values\n",
    "        optimizer.step()\n",
    "    \n",
    "        # visual output of the progress\n",
    "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    # tell the model that we are in the testing phase\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # tell PyTorch not to update the gradients while doing the following\n",
    "    with torch.no_grad():\n",
    "        # iterate over the test Data Loader, with one data batch every iteration\n",
    "        for data, target in test_loader:\n",
    "            # initialize the testing input data and labels to the device memory\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # run the test data through the model\n",
    "            output = model(data)\n",
    "            # compute the loss and accumulate the loss over all batches of test data\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            # identify the output prediction as the index with the highest\n",
    "            # log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            # if the prediction is the same as the expected (target),\n",
    "            # track it in the \"correct\" counter\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # compute the average loss over the test dataset\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # print a summary of the test run\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0FYVWkGOFBS",
    "outputId": "3bab1253-068e-41aa-a5af-34be9e25b98e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]/tmp/ipykernel_3804/4093577519.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "loss=0.010093619115650654 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 23.15it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0243, Accuracy: 9938/10000 (99.38%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup the SGD optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# multi-epoch train-test loop (currently shows only 1 epoch)\n",
    "for epoch in range(1, 2):\n",
    "    # train the model, results in model parameters to be adjusted\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    # test the model accuracy with the learnt model parameters\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6agTEkqzz6TZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]/tmp/ipykernel_3804/4093577519.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "loss=0.00026630351203493774 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0263, Accuracy: 9921/10000 (99.21%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.0004544091643765569 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0256, Accuracy: 9942/10000 (99.42%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.00015562029147986323 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0278, Accuracy: 9938/10000 (99.38%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=1.7085976651287638e-06 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0281, Accuracy: 9941/10000 (99.41%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=2.8099166229367256e-05 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0302, Accuracy: 9939/10000 (99.39%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]/tmp/ipykernel_3804/4093577519.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "loss=0.02342689037322998 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.72it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0302, Accuracy: 9934/10000 (99.34%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.000468453363282606 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.62it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0294, Accuracy: 9922/10000 (99.22%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.00047423309297300875 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0435, Accuracy: 9905/10000 (99.05%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.0004801483592018485 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0265, Accuracy: 9926/10000 (99.26%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.0035322632174938917 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0277, Accuracy: 9942/10000 (99.42%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]/tmp/ipykernel_3804/4093577519.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "loss=4.304241156205535e-05 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0260, Accuracy: 9945/10000 (99.45%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=6.427519110729918e-06 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0276, Accuracy: 9948/10000 (99.48%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=8.755607268540189e-06 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0287, Accuracy: 9948/10000 (99.48%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=1.3946876606496517e-05 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0295, Accuracy: 9948/10000 (99.48%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=1.2558134585560765e-05 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0301, Accuracy: 9948/10000 (99.48%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]/tmp/ipykernel_3804/4093577519.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "loss=3.104405621456863e-08 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0312, Accuracy: 9949/10000 (99.49%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=7.562179575870687e-07 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0320, Accuracy: 9950/10000 (99.50%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=1.3535178311485652e-07 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0327, Accuracy: 9950/10000 (99.50%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=2.0646470147767104e-05 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0332, Accuracy: 9951/10000 (99.51%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=7.450578376477779e-09 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 23.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0337, Accuracy: 9951/10000 (99.51%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.04, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]/tmp/ipykernel_3804/4093577519.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "loss=1.8129628642782336e-07 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0345, Accuracy: 9952/10000 (99.52%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=9.934105982267738e-09 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 23.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0352, Accuracy: 9953/10000 (99.53%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=1.0517297823753324e-06 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0358, Accuracy: 9953/10000 (99.53%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=1.539778651249435e-07 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0363, Accuracy: 9953/10000 (99.53%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=1.138670313594048e-06 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 23.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0367, Accuracy: 9953/10000 (99.53%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.08, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
